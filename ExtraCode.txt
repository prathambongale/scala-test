println("mongo record count : " + rdd.count)
    println("mongo list : " + rdd.collect().foreach(Document=>Unit))
    println("mongo first record : " + rdd.first().toJson())
    // Print All MongoDB collection record
    println("Collection Record : ")
    rdd.foreach(x => println(x))
    
    // Print Schema
    rdd.toDF().printSchema()

    // DataFrames can be saved as Parquet files, maintaining the schema information

    rdd.toDF().write.parquet(args(0))

    // Read in the parquet file created above
    // Parquet files are self-describing so the schema is preserved
    // The result of loading a Parquet file is also a DataFrame
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    val parquetFileDF = sqlContext.read.parquet("people.parquet")

    // Parquet files can also be used to create a temporary view and then used in SQL statements
    parquetFileDF.createOrReplaceTempView("parquetFile")
    val namesDF = sqlContext.sql("SELECT * FROM parquetFile")
    //namesDF.map(attributes => "Name: " + attributes(0)).show()
    namesDF.show()
